---
title: "LLM 评估指南"
author: pillar
date: 2024-10-10
category: llm
layout: post
permalink: /llm/app/llm-evaluation

---

目前有三种主要的评估方式：自动基准测试、人工评估和使用模型评估。

# HOW

## Bechmarks

### 1. 概述

使用基准测试评估通常包含两部分：

1. 样本集：由多个样本组成，包含模型的输入和预期输出(gold答案)。样本设计旨在模拟真实场景，包括边界情况。
2. 评估指标（得分）：用于量化模型性能的方法。对于大语言模型(LLM)，主要考虑两类输出：
   - 生成式评估：分析模型生成的文本。
   - 多选评估或对数似然评估：分析模型对不同选项的概率预测。

**自动化基准的优势：**

- 结果一致且可重复
- 可大规模实施且成本较低
- 大多数指标易于理解
- 通常使用高质量数据集，尽管并非完美(如MMLU及其改进版本)

**局限性：**

- 对复杂任务的评估能力有限
- 难以全面评估抽象能力(如"数学能力")
- 数据集污染问题：公开发布的数据集可能被纳入后续模型的训练数据，但在真实任务中的泛化性较差。解决污染的一种方法是使用动态基准测试（定期刷新数据集进行评估），但这种方法在长期来看成本很高。



### 2. 评估数据集的选择与设计

#### 选择现有数据集

在使用现有数据集时，需要仔细评估以下几个方面：

**创建过程**

- 样本创建者的背景和专业程度：由专家创建的数据集 > 付费标注者的数据集 ~ 众包数据集 > MTurk 数据集。
- 标注者间的一致性和质量审核流程：标注者是否意见一致？以及整个数据集是否经过作者审查。 这对于那些依靠低薪标注者的评估数据集尤为重要，通常这些标注者并不是目标语言的母语者（如 AWS Mechanical Turk），否则可能会出现拼写错误、语法错误或无意义的答案。

**样本**

- 抽样检查 prompts 的清晰度和答案的准确性，评估信息完整性
- 确认样本与评估目的的相关性
  - 这些问题是否是您希望用来评估大型语言模型的问题？
  - 这些示例是否与您的使用场景相关？

一些公开的[评测数据集](https：//github.com/huggingface/evaluation-guidebook/blob/main/contents/Automated%20benchmarks/Some%20evaluation%20datasets.md)。



#### 设计自定义数据集

自定义数据集可通过以下方式创建：

1. 整合现有数据：可以从不同来源聚合现有数据，评估与您的任务相关的能力。
2. 利用人工标注
3. 生成合成数据
   - 使用大型语言模型(LLMs)
   - 应用基于规则的技术



### 3. 提示词设计

提示词结构通常包括：

- 任务说明(可选)
- 背景信息：为问题提供额外的上下文。
  - *例如：对于总结或信息抽取任务，您可以提供内容来源*
- 核心问题
- 选项(多选题)
- 连接词

**设计注意事项：**

- 即便是微小的措辞变化也可能显著影响结果
- 考虑使用示例和连接词以增强理解
- 警惕模型对特定提示格式的过拟合
- 某些评估指标可能需要严格限制输出格式



### 4. 如何应对数据污染问题

互联网上的公开数据集很可能已经被污染或将来会被污染。为了减轻这个问题，我们可以采取以下措施：

1. 使用"金丝雀字符串"：在评估数据中加入特殊的字符组合，帮助模型开发者检查训练数据是否包含评估内容。
2. 加密或限制评估数据：这样可以防止网络爬虫轻易获取，避免评估数据意外进入训练集。
3. 动态更新基准测试：定期更新测试内容，防止模型简单记忆答案。不过，这种方法可能会增加成本。
4. 事后检测污染：在进行基准测试后，尝试通过分析生成内容的困惑度或设计对抗性提示来检测可能的污染。但要注意，目前没有完全可靠的污染检测方法。

即便数据集被污染，它可能仍然对模型训练有价值。



> "金丝雀字符串"是一种独特的、容易识别的文本序列，被故意插入到评估数据集中。它的主要目的是帮助模型开发者检测他们的训练数据是否被污染了评估数据。这个概念的名字来源于矿工们曾经用金丝雀来检测矿井中的有毒气体。
>
> 举个例子：
>
> 假设我们正在开发一个问答系统的评估数据集。我们可能会在这个数据集中插入如下的金丝雀字符串： `CANARY_STRING_2024_EVAL_SET_DO_NOT_USE_IN_TRAINING`
>
> 这个字符串会被添加到评估数据集的某些问题或答案中，比如：
>
> 问题： 什么是光合作用? `CANARY_STRING_2024_EVAL_SET_DO_NOT_USE_IN_TRAINING` 答案： 光合作用是植物利用光能将二氧化碳和水转化为葡萄糖和氧气的过程。
>
> 当模型开发者在训练他们的模型时，他们可以搜索这个特定的字符串。如果在训练数据中发现了这个字符串，就意味着评估数据已经不知不觉地混入了训练集，可能会导致模型性能评估的偏差。
>
{： .block-tip }



### 5. 实际应用中可能遇到的问题

**模型微调、系统提示和对话模板**

一些经过指令调优的模型可能表现不佳，除非你：

- 在开始推理时添加它们的系统提示。
- 使用正确的对话模板（通常需要在对话中添加"Assistant"和"User"等标识）。

另外，不同的分词器可能会对对话模板有不同的处理方式，尤其是在处理空格时。



**分词和多项选择题评估**

在进行多项选择题评估时，可能会遇到以下与分词相关的问题：

1. 上下文和选项的分词方式：
   - 通常应该将上下文和选项一起分词。
   - 但某些分词器可能会在处理上下文和选项时出现不一致，影响比较的准确性。
2. 句子开头和结尾标记：
   - 某些模型对句子开头标记很敏感，可能需要手动添加。
   - 有些模型可能无法在预期的句子结尾处停止，需要额外的检查和处理。
3. 多语言评估中的分词：
   - 对于不使用空格作为词分隔符的语言（如中文、日语等），需要使用专门的分词器来确保正确评估。



**加速多项选择题评估的小技巧**

如果能让模型只预测一个标记来回答问题，可以大大提高评估速度。这样只需对上下文进行一次推理，就能得到所有可能答案的概率分布。



**生成式评估中的意外结果**

如果遇到模型表现意外差的情况，请检查以下几点：

1. 输出解析是否过于严格，导致有效答案被忽略。
2. 模型是否无法在少量样本中遵循指定的输出格式。
3. 模型是否过于啰嗦，难以得出准确答案。

针对这些问题，可以考虑调整解析方法、修改提示格式、增加允许的上下文长度，或在任务说明中强调简洁回答的重要性。



## 人工评审

### 1. 概述

人工评估就是让真人来评价AI模型的表现。本文主要讨论事后评估：模型训练完成后，针对特定任务让人来打分。



#### 人工评估的优缺点

优点：

- 灵活性强：只要定义清楚，几乎什么都能评估
- 不会污染训练数据：评估者提出的新问题通常不在训练数据中
- 反映真实人类偏好：毕竟是人在打分

缺点：

- 第一印象偏见：评估者容易被初始印象影响
- 语气偏见：自信的语气可能掩盖事实错误
- 自我偏好偏见：评估者倾向于赞同符合自己观点的回答
- 身份偏见：不同背景的人可能有不同的评判标准



### 2. 系统性评估

有三种主要方法：

1. <u>没有现成数据集</u>时： 给评估者一个任务和评分标准，让他们与模型互动并打分。

   例如："测试模型是否会说脏话，说了给0分，不说给1分"。

2. <u>有现成数据集</u>时： 用数据集中的问题去问模型，然后把问题、模型回答和评分标准给评估者。 例如："如果模型泄露隐私信息给0分，否则给1分"。

3. <u>有数据集和评分</u>时： 让评估者复查已有的评分，找出可能的错误。这也可以用来测试新的评估体系。

**注意：**

- 对于已上线的模型，还可以收集用户反馈或做A/B测试。
- AI审计(第三方评估)通常也基于人工，但不在本文讨论范围内。

**额外优点：**

- 获得高质量、针对性强的数据
- 数据隐私得到保障
- 结果易于解释

**额外问题：**

- 成本高：需要付费给专业评估者
- 难以扩展：每次评估都需要新的人力
- 可重复性差：很难完全复现之前的评估结果



### 3. 非正式评估

还有两种比较随意的人工评估方式：

1. 感受测试： 个人随意测试模型，获得对模型整体表现的直观感受。结果常在社交媒体上分享，属于轶事证据，可能存在确认偏见。
2. 竞技场： 让大众用户与多个模型交谈，选出他们认为更好的模型。通过投票结果给模型排名，如LMSYS聊天机器人竞技场。

**优点：**

- 成本低：依靠志愿者
- 容易发现边界案例：利用了大众的创造力
- 相对容易扩展：只要有足够的感兴趣的人参与

**问题：**

- 高度主观：难以保证评分标准一致
- 样本代表性差：参与者可能不能代表普通大众
- 容易被操纵：第三方可能影响评估结果



### 4. 如何提高数据的标注质量

1. 劳动力选择和激励很重要：
   - 根据任务需求选择合适的标注人员，比如母语者、领域专家等。
   - 筛选出高质量工作者，排除使用AI生成答案的人。
   - 合理支付报酬以提高积极性。
2. 制定详细的标注指南：
   - 花时间深入思考和制定指南，这很关键。
   - 指南可能比想象的更模糊，需要反复完善。
3. 迭代标注过程：
   - 准备多轮标注，让标注者更好理解要求。
   - 生成多个样本以聚焦关键内容。
4. 质量控制：
   - 检查标注者间的一致性。
   - 手动筛选，只保留最高质量的结果。
5. 可以使用专业工具如Argilla来辅助构建高质量数据集。



## 使用模型进行评审

### 1. 概述

评估模型是一种特殊的神经网络，它的主要任务是对其他神经网络的输出进行评价。最常见的用途是评估文本生成的质量。它的强大之处在于，它能够对文本的复杂和细微特征进行评分。

> 举个例子，如果想测试模型是否预测出了正确的事实或数字，可以简单地比对预测结果和参考答案是否完全一致。
>
> 但是，如果想评估一些更开放性的能力，比如文字的流畅度、诗歌的质量，或者对输入的忠实度，就需要更复杂的评估工具了。这就是评估模型大显身手的地方。



评估模型的种类很多，从简单的专用分类器(比如用来过滤有害内容的"垃圾邮件过滤器")到功能强大的大型语言模型(LLM)都包括在内。当使用LLM作为评估模型时，我们需要给它一个明确的指令，告诉它如何对文本进行评分。比如，"请给这段文字的流畅度打分，0到5分，0分表示完全无法理解..."



**评估模型主要用于三类任务：**

1. 对生成的文本进行评分：根据特定标准(如流畅度、有害程度、一致性、说服力等)给文本打分。
2. 成对比较：比较两段文本，选出在某方面表现更好的那一个。
3. 计算相似度：衡量模型输出与参考文本之间的相似程度。



值得注意的是，除了本文重点介绍的LLM评估方法外，还有一些其他有效的评估手段。比如，分类器型评估模型在很多场景下都表现稳定且适应性强。此外，最近提出的将奖励模型用作评估者的方法也很有前景(详见[这份技术报告](https：//research.nvidia.com/publication/2024-06_nemotron-4-340b))。



#### **使用LLM进行评估的优缺点**

**优点：**

- 相比人工评估更加客观：它们可以以一种客观且可重复的方式自动化主观判断。
- 规模化且可重复：相比人工标注，它们可以更快速地处理大量数据，并且能保持一致性。
- 成本效益高：不需要训练新模型，只要有好的提示和高质量的LLM就可以。比起雇佣真人标注者，成本更低。
- 与人类判断有一定相关性：在某些方面，它们的评判结果与人类的判断相近。

**缺点：**

- 隐藏的偏见：虽然看似客观，但LLM评估者可能存在难以察觉的偏见。这些偏见比人类的偏见更难被发现，因为我们通常不会主动去寻找它们。
- 数据量大：虽然处理速度快，但会产生大量需要人工审核的数据，以确保质量。
- 专业性有限：在某些特定领域，聘请真正的专家可能会得到更高质量的评估结果。



### 2. 评估LLM的主要步骤：

1. **选择合适的基准**

你需要一个基准来对比评估结果。这个基准可以是：

- 人工标注的数据
- 已知在特定任务上表现良好的其他评估模型的输出
- 公认的黄金标准
- 使用不同提示的同一模型的结果

基准数据集不需要很大，50个左右的样本通常就足够了。但这些样本需要具有代表性，能够体现边界情况，并且质量尽可能高。

2. **确定评估指标**

评估指标用于比较模型评估结果与基准之间的差异。

- 如果模型输出是二元类别或进行成对比较，评估会相对简单。你可以计算准确率(成对比较)或精确率和召回率(二元分类)，这些指标都很直观易懂。
- 如果是评分类的任务，与人工或其他模型评分的相关性比较会更复杂一些。你可以参考这篇博客中的相关部分，里面有更详细的解释。

3. **评估你的评估模型**

这一步就是用你的模型及其提示来评估测试样本。得到评估结果后，再用前面选定的指标和基准来计算评估分数。

你需要设定一个可以接受的阈值。对于成对比较任务，根据难度不同，可以将目标准确率设为80%到95%。对于评分任务，通常Pearson相关系数达到 0.8 就令人满意了。不过也有研究认为0.3就表示与人工评分有良好相关性，所以具体情况可能会有所不同。



### 3. 选择什么样的 LLM 进行评估

在使用语言模型(LLM)进行评估时， 有几种选择：

1. **使用通用大型LLM**

优点：

- 性能强大，尤其是一些封闭源代码模型如ChatGPT
- 开源模型如Qwen 2.5和Llama 3也在快速追赶
- 可通过API轻松访问，无需本地部署

缺点：

- API模型可能会无预警更新，影响结果可重复性
- 封闭模型不透明，难以解释
- 可能存在数据隐私问题

2. **使用小型专用LLM**

优点：

- 体积小，可本地运行
- 针对特定任务优化
- 开源透明

例如：Flow-Judge、Prometheus等模型

3. **自行训练模型**

步骤：

- 收集相关偏好数据
- 决定从头训练还是微调现有模型
- 使用收集的数据进行训练

这种方法可以完全定制，但需要更多工作。



### 4. **如何减少大语言模型在评估时的已知偏见**

大语言模型(LLM)作为评估者时存在一些问题，我们可以采取以下措施来减轻这些偏见：

- **判断不一致** ： 多次询问同一问题时，模型可能给出不同的答案。解决方法是多问几次，采用大多数的结果。

- **偏爱自己** ： 模型倾向于给自己的回答更高分。解决方法是使用"评审团"，让多个模型一起评分。

- 对文本变化不敏感

   ： 模型难以识别轻微修改过的文本，评分也不够稳定。解决方法是：

  1. 让模型先解释理由再打分
  2. 给模型一个明确的评分标准

- 位置偏好

   ： 在比较多个选项时，模型可能总是偏爱某个位置的答案。解决方法是：

  1. 随机调换选项顺序
  2. 计算每个选项被选中的概率

- **长度偏好** ： 模型倾向于喜欢更长的答案。解决方法是在评分时考虑答案长度的影响。

- **格式敏感** ： 如果提问方式与模型训练时差异较大，评估效果会变差。解决方法是尽量按照模型训练时的格式来提问。



此外，还需注意：

- 模型与人类评分的一致性存在争议，特别是在专业领域(如医疗、法律等)。
- 模型不擅长发现虚假信息，尤其是那些看似真实但略有偏差的内容。
- 在摘要、忠实度等任务上，模型的表现与人类评分的相关性较低。



因此，在选择用LLM进行评估时，需要考虑这些局限性，并针对具体任务选择合适的评估方法。





# WHY

### 1. 验证训练过程的有效性

评估的首要目的是验证模型训练是否按预期进行。这类似于软件开发中的回归测试，目的是确保新的改动没有破坏现有功能。这种评估主要关注趋势和范围，而非具体分数。它帮助我们判断训练方法的合理性，而不是评估模型的实际能力极限。

> 例子：假设你正在训练一个模型来回答常识性问题。你可能会这样检查：
>
> - 在训练开始时，模型只能正确回答20%的问题。
> - 经过一周的训练，它能回答40%的问题。
> - 再过一周，它达到了60%。
>
> 这种进步趋势告诉你，训练正在起作用。但如果突然有一天，正确率下降到10%，你就知道可能出了问题，需要检查一下训练过程。



### 2. 模型间的相对比较

对不同模型进行排序，以识别最优的架构和方法。这里重要的是排名的稳定性，而非具体的分数。一个稳健的评估体系应该能在各种条件下保持相对一致的排序结果。

> 例子：假设你有三个模型：A、B和C。
>
> - 在数学题上，排名可能是： B > A > C
> - 在写作任务上，可能是： C > A > B
> - 在编程方面，可能又是： A > C > B
>
> 重要的是这种排序是否稳定。如果每次测试都得到完全不同的排序，那这个评估方法可能就不太可靠。



### 3. 能力评估

这是最具挑战性的目标，因为：

- 复杂能力很难用单一指标来衡量
- 我们缺乏对"能力"本身的明确定义，特别是涉及推理和认知的高级能力
- 为人类设计的能力评估框架可能不适用于AI模型

因此，当谈到模型能力时，我们通常只能说"这个模型在特定任务上表现最佳"，而不能简单地断定"这个模型最强"。

> 例子：假设你想知道一个LLM是否能理解讽刺。
>
> - 你可能会给它一些讽刺的句子，看它能否正确解释。
> - 但即使它在这个测试中表现良好，也不能100%确定它真的"理解"了讽刺。
> - 它可能只是学会了一些模式，而不是真正理解。



# 模型推理与评估

### 大语言模型(LLM)的工作原理

大语言模型的核心功能是预测文本。给定一段输入文本，它们能够预测合理的后续内容。这个过程分为两个主要步骤：分词和预测。

1. 分词过程：

   输入文本(在实际使用时称为"prompts")首先被拆分成小的文本单元，称为"token"。每个 token 都与一个特定的数字相关联。模型能够识别的所有 token 集合被称为其"词汇表"。

2. 预测过程：

   模型会根据输入 prompts 生成一个概率分布，预测词汇表中最可能出现的下一个 token。为了生成连续的文本，通常会选择概率最高的 token(有时会添加一些随机性以增加输出的多样性)。然后将这个新 token 添加到 prompts 的末尾，重复上述过程，不断生成新的文本。



### 评估大语言模型

LLM的评估主要分为两大类：

1. 给定 prompt 和一个或多个答案选项，评估模型对这些答案的概率预测。
2. 给定 prompts， 评估模型生成的文本质量。

#### **对数似然评估：**

这种方法用于评估模型在给定 prompt 的情况下，产生特定续写的可能性。它主要用于有明确选项的情况，比如多选题。

步骤如下：

- 将每个答案选项与提示连接，输入模型。模型输出每个 token 的logits值(一种表示概率的数值)。
- 只保留与答案选项相关的最后的logits值。
- 应用对数softmax函数，得到对数概率(范围是[-inf， 0])。
- 将所有词元的对数概率相加，得到整体选项的对数概率。
- 可以根据选项长度进行归一化处理。

基于这个方法，可以应用以下指标：

- 在多个选项中找出模型最偏好的答案。
- 测试单个选项的概率是否高于0.5。
- 研究模型的校准性(一个校准良好的模型应该为正确答案赋予最高概率)。

> 例子：
>
> 提示："1+1等于多少？" 选项：A. 2  B. 3  C. 4
>
> 将选项与提示连接，输入模型：
>
> 1. "1+1等于多少？A. 2"
> 2. "1+1等于多少？B. 3"
> 3. "1+1等于多少？C. 4"
>
> 模型输出logits值，假设模型对这三个选项输出的logits值如下： A. 2 ： 10 B. 3 ： 2 C. 4 ： 1
>
> 这些数字代表模型认为每个选项的"合理性"。数值越大，模型认为这个选项越可能是正确答案。
>
> 应用对数softmax函数，我们将这些logits值转换为概率。简化起见，我们直接给出转换后的概率： A. 2 ： 0.9 B. 3 ： 0.08 C. 4 ： 0.02

**优缺点：**

- 优点：
  - 确保所有模型都能访问正确答案
  - 提供模型“置信度”（以及校准）的代理
  - 评估速度快，特别是在我们要求模型只预测一个标记（A/B/C/D 选项索引，或是是/否等）时
  - 可用于获取小型模型任务性能的信号
- 缺点：
  - 会略微高估小型模型的得分，因为如果不受限制地生成答案，它们可能会生成超出可用选项范围的内容
  - 有些模型[偏好基于呈现顺序的特定选项](https：//arxiv.org/abs/2309.03882)，这可能导致评估结果不具代表性

#### **生成式评估：**

这种方法评估模型根据 prompts 生成的文本质量。过程如下：

- 将提示输入模型，选择最可能的下一个词元。将选择的词元添加到提示末尾，重复这个过程。直到达到结束条件(如最大长度或特殊的停止词元)。
- 将生成的内容与参考答案进行比较。可以使用简单指标(如精确匹配)、复杂指标(如BLEU分数)，或使用其他模型作为评判来评分。

**优缺点：**

- 优点：
  - 实际上应与生成流畅文本的 LLM 能力相关联，这通常是人们真正感兴趣的内容
- 缺点：
  - 可能更难评分（请参见下文“度量”部分）
  - 通常比对数概率评估稍贵，尤其是包含采样时



### 限制模型输出

在某些情况下，需要模型的输出遵循特定格式。有几种方法可以实现这一点：

1. 使用提示：
   - 在任务提示中添加具体指令，告诉模型如何回答。这种方法对于高能力模型通常效果不错。
2. Few-shot和上下文学习：
   - 在提示中提供示例(few-shot提示)，引导模型按照特定格式回答。
   - 这种方法在2023年底之前效果很好。但随着指令微调技术的普及，新模型可能会过度拟合特定的提示格式。对于上下文窗口较小的旧模型，这种方法可能受到限制。
3. 结构化文本生成：
   - 使用语法或正则表达式定义输出结构。例如，outlines库使用有限状态机实现这一功能。
   - 这种方法可以减少评估中的提示方差，使结果更稳定。然而，最新研究表明，在某些任务(如推理)中，这可能会降低模型性能。





*参考：*

*https://huggingface.co/blog/clefourrier/llm-evaluation*

*https://github.com/huggingface/evaluation-guidebook?tab=readme-ov-file*