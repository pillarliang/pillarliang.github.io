{"/llm/app/multimodal-doc": {
    "title": "多模态文档检索",
    "keywords": "llm",
    "url": "/llm/app/multimodal-doc",
    "body": "概述 本文主要探讨多模态文档检索的方案。对传统 RAG，CLIP，Colpali 三种方案依次介绍。 RAG 基本流程基本： 首先就是要进行文档解析及存储，为了避免 garbage in garbage out，这一步对于后面的生成至关重要。 首先看下一般情况下是怎么对文档进行处理的。 传统 RAG 传统 RAG 中对文档解析是一件很麻烦的事，涉及多种文档以及解析策略。 主要面临的挑战如下： 处理 PDF 文档数据特别复杂 需要 OCR 和 layout detection（需要花时间去修正OCR 识别后的布局信息，OCR 有些有些布局识别错误） 处理表格和图形等视觉元素的困难。（比如文本高亮；图形倒是可以通过 jina clip 做 embedding） 太慢了… 通常这种方式主要涉及四阶段的处理流程: 文档预处理: 针对不同格式的文件做不同的处理 文档内容解析: 包括布局分析、公式检测、OCR、公式识别和表格识别等步骤 文档内容后处理（layout recognition）: 移除无效区域 - 根据区域定位信息拼接内容 - 获取不同文档区域的定位、内容和排序信息 格式转换: 生成所需的各种格式 (如 Markdown) 使用 paddleOCR 进行文档解析生成 markdown ：https://github.com/pillarliang/TextParse 既然文件中的图片可以用多模态 embedding，那可以试试直接将文档页面转为 Image 做 embedding。 CLIP Contrastive Language–Image Pretraining，能够同时理解图像和文本，并在两者之间建立联系，实现了图像和文本之间的跨模态理解。 CLIP的基本原理是对比学习，即让模型学习区分正样本（匹配的图像和文本对）和负样本（不匹配的图像和文本对）。 为了实现这一目标，CLIP使用了一个多模态编码器，它由两个子编码器组成：一个用于图像，一个用于文本。 图像编码器可以是基于卷积神经网络（CNN）或者视觉变换器（ViT）的模型， 文本编码器则是一个基于Transformer的模型。 这两个编码器都可以将输入转换为一个固定长度的向量表示，然后通过计算向量之间的余弦相似度来衡量图像和文本之间的匹配程度。 目前开源的做的最好的是 Jina-clip（到24.09为止） Jina-CLIP 论文链接：https://arxiv.org/abs/2405.20204 项目链接：https://huggingface.co/jinaai/jina-clip-v1 是什么？ Jina-CLIP 结合了 OpenAI 的 CLIP 模型。jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks. 用于多模态搜索和匹配任务，比如在图像搜索中，用户可以输入文本查询，然后系统会返回与该文本语义上相关的图像。这样的系统对需要处理大量图像和文本数据的应用非常有用，比如电子商务、数字资产管理和社交媒体平台等。 Jina-CLIP 可以用于创建高效的搜索引擎，支持跨模态的检索，帮助开发者快速构建复杂的 AI 应用。Jina AI 提供的工具和框架使得开发者能够更容易地集成和部署这样的模型，从而加速 AI 产品的开发过程。 怎么用？ 因为 Jina 对文本和图片同时 embedding，对于召回的文本和图片，相似度分数不在一个维度，需要有一个超参数来平衡文本和图片分数。超参数如何设定，Jina 自己有提到。 在 RAG（Retrieval-Augmented Generation）系统中，如果检索到的结果是图片，可以通过以下方法将图片信息应用于生成阶段： 图片描述生成（Image Captioning）： 使用图像描述生成模型（如基于深度学习的模型）将检索到的图片转换为文本描述。这样的模型可以通过分析图像的内容生成一段描述性文字。 将生成的文本描述作为上下文输入到文本生成模型中，以帮助生成更相关的响应。 特征提取和嵌入 ： 使用图像编码模型（如 CLIP 或其他卷积神经网络）提取图像的特征向量。 将这些特征向量与文本特征结合，作为生成模型的输入。这需要生成模型能够接受多模态输入（图像和文本）。 知识库补充： 将图片中的信息转化为结构化数据（例如，通过对象检测、场景理解等技术），补充到知识库中。 在生成阶段，利用这些结构化信息作为背景知识来丰富生成内容。 对话上下文增强： 在对话系统中，利用图片的文本描述或特征向量来增强对话上下文。 生成模型可以根据这些上下文信息生成更具针对性的回复，特别是在需要描述或讨论图片内容时。 视觉提示（Visual Prompting）： 如果生成模型支持视觉提示，可以直接使用图像或其特征作为提示，指导生成模型输出与图片相关的内容。 这些方法的选择取决于具体应用场景和技术实现的复杂性。在实现过程中，可能需要结合多个技术栈（如计算机视觉、自然语言处理）以及合适的模型架构，以实现最佳效果。 获取到检索的图像后，可采取两种方案： 图片描述生成 LLM vision prompting 具体两种方案的 速度、accuracy、cost 需要进行实验。 同时在多轮对话中验证，多轮对话对于同一图片处理多次。 Jina-CLIP 接受的 image是：URL 或本地图片，不接受 base64。因为内置的 encode_image 就是将 图像转为 base64. openai 接受：网络 URL 和 base64. 但是，文档中往往涉及很多不同的类型的数据，直接转为一张 Image 用 Jina clip 做 embedding 来是能力欠缺。这种方式表现出信息损失：要么文本信息丢失，要么表格被误解，要么格式的重要性丢失。 code &amp; demo: ColPali 多模态文档检索模型 ColPali 通过直接处理文档图像来解决这些问题，消除了复杂预处理的步骤，实现更快、更准确的检索。 https://github.com/illuin-tech/colpali https://huggingface.co/vidore/colpali 是什么/Architecture？ 利用 VLMs （PaliGemma）的文档理解能力，仅通过将文档页面转为图像生成高质量上下文 embedding。结合（ColBERT） late interaction 匹配机制，ColPali 大幅超越现代文档检索流程，同时速度更快且可端到端训练。(下面会有ColBERT 的介绍) It is a PaliGemma-3B extension that generates ColBERT- style multi-vector representations of text and images. PaliGemma 通过将图像分割成固定大小的块（patches），然后对这些块进行编码以生成图像特征表示。设计目标之一是确保图像和文本之间的多模态对齐，从而在同一向量空间中表达这些不同类型的数据。 ColPali 的核心是基于 PaliGemma-3B 模型，该模型结合了： A SigLIP-So400m/14 vision encoder A Gemma-2B language model ColPali 架构通过添加以下内容扩展了 PaliGemma： 一个投影层将语言模型嵌入映射到低维空间（D=128） 一种受 ColBERT 检索模型启发的 late interaction 机制 工作原理 离线索引阶段 每个文档页面都通过视觉编码器 （SigLIP） 送入 生成的图像补丁嵌入由语言模型 （Gemma-2B） 处理 投影层将输出映射到低维空间 生成的嵌入内容将存储为文档页面的多向量表示形式 在线查询阶段 使用 LM 对 query 做 embedding 后期交互机制计算 query tokens 和 document patches 之间的相似性分数 系统根据这些分数返回最相关的文档 特点 仅使用视觉特征的高效文档索引 在文档检索任务有最好性能 处理各种文档类型的能力，包括文本、表格和图形 端到端可训练架构 查询阶段的低延迟 Advantages: Speed: ColPali 的索引过程比传统方法快得多，因为它绕过了复杂的预处理步骤。 Accuracy: 通过利用视觉特征，ColPali 在文档检索中实现了更高的准确性，特别是对于视觉丰富的文档。 Flexibility: ColPali 可以处理各种文档类型和语言，无需修改。 Efficiency: The late interaction mechanism allows for fast querying even with large document corpora. Interpretability: ColPali 提供了可视化，显示哪些图像块对检索决策贡献最大，从而增强了可解释性。 Benchmark https://github.com/illuin-tech/vidore-benchmark https://medium.com/@simeon.emanuilov/colpali-revolutionizing-multimodal-document-retrieval-324eab1cf480 应用 byaldi 一个封装了 colpali 接口的库。 Byaldi索引默认是存储在文件中的，如果要存储到数据库中，就不能用了。 Code &amp; demo: ColBert| Contextualized Late Interaction BERT WHY：传统向量搜索： 但将信息从数百个 token 压缩到单个向量必然会丢失信息。 传统的向量搜索依赖于将查询和文档表示为固定维度的向量，并通过计算相似度来匹配。这种方法缺乏对具体词汇的直接解释，因为用户只能看到相似度分数，而不能追踪到哪些具体的词或特征影响了这个分数。 向量生成通常是通过复杂的嵌入模型，这些模型的内部机制对用户来说是“黑箱”的，难以理解。 WHAT： ColBERT是对 BERT 的一种改进，特别是在信息检索和问答系统等任务中。multi-vector representations of text and images ColBERT的主要特点是： 保留 Token-wise Representation： 将 query 和 document tokenize，然后对 token embedding 延迟交互 ：ColBERT引入了一种“延迟交互”的机制，在检索过程中，先计算每个query 的 token 表示，然后计算 query tokens 和 document tokens 之间的相似性得分。将所有这些 token 级别的最大得分相加，生成整体的 MaxSim 相关性得分，从而找出最相关的文档。 MaxSim: 每一个 query token 都与 document 中 token 计算得分，获取得分最高的 token。 虽然 late interaction 方法相对于简单的整合向量计算有更高的计算复杂度，但其线性复杂度 O(m×n) 使得实际应用依然是可行的，并带来了更好的效果。 对于一个给定的 512 个 toekn 的文档，我们似乎存储了512倍的数据? 第一个是之前提到的降维层。 第二个是激进的量化：ColBERT向量可以压缩到2位，并保持超过99%的性能！ 最近的研究表明，还可以进行小规模的池化，以进一步减少50%到75%的存储，同时几乎没有性能损失。"
  },"/llm/app/neural-network": {
    "title": "神经网络",
    "keywords": "llm",
    "url": "/llm/app/neural-network",
    "body": ""
  },"/llm/app/agentic-patterns": {
    "title": "Agentic Design Patterns",
    "keywords": "llm",
    "url": "/llm/app/agentic-patterns",
    "body": "我们要挖掘和利用 LLM 涌现出的能力。利用 LLM 能力(也可以概括为 Agent)的两种方式如下，： In context leanring Chain of thought 提高 GPT-4 和 GPT-3.5 性能的四种 AI agent策略 Reflection: LLM 检查自己的工作，以寻找改进的方法。 Tool Use: LLM 被赋予了诸如网络搜索、代码执行或其他任何功能的工具，以帮助其收集信息、采取行动或处理数据。 Planning: LLM 制定并执行一个多步骤计划以实现目标（例如，撰写论文大纲，然后进行在线研究，然后撰写草稿，等等）。 Multi-agent collaboration: 多个 AI Agent 共同工作，分配任务并讨论和辩论想法，以提出比单个Agent 更好的解决方案。 1. Reflection 这种模式允许大型语言模型（LLMs）对其输出进行反思和批评，遵循以下步骤： LLMs 生成一个候选输出。 LLMs 对之前的输出进行反思，提出修改、删除、改进写作风格等建议。 LLMs 根据反思对原始输出进行修改，然后开始新一轮迭代…… 在使用 ChatGPT 时，如果 reponse 不满意，我们会提供反馈来帮助 LLM 改进其输出，获得更好的 response。这里目的是：自动化提供反馈的过程。 情景：撰写行业简评。在生成初稿后，agent 对其进行阅读，找出需要修改的部分，然后通过反复优化进行完善。 code: demo: 2. TOOL USE LLM被赋予可以请求调用以收集信息、采取行动或操纵数据的功能。 具体过程：LLM要么经过微调，要么被提示（可能是通过少量提示）生成一个特殊字符串，如 {tool: web-search, query: \"咖啡机评论\"} （字符串的确切格式取决于实现。）以请求调用搜索引擎。当找到搜索引擎函数时时，使用相关参数调用该函数，并将结果作为额外输入上下文传递回LLM以进行进一步处理。 情景：为 Instagram 等平台生成旅行指南。Agent 可以搜索有关当地天气、交通路线和景点开放时间的实时文本和视觉信息，同时根据作者的风格和平台标准编辑和格式化内容，从而实时制作高质量的内容。 code: demo: 3. Planning Mode Agent 动态将复杂任务分解并按照计划执行。另一方面，它导致结果的可预测性降低。 背景：大型模型的生成效果取决于训练数据的有效性，有时可能由于幻觉而产生次优结果。 规划模式允许 agent 基于计划的任务步骤对生成的内容进行反复优化和处理，从而产生更高质量的输出。 4. Multiagent Collaboration Mode 多个AI Agent 协同工作，分配任务、讨论和辩论想法，以提出比单个 Agent 更优的解决方案。 背景：大型模型有时会遇到需要团队合作才能完成的系统性任务，而单个 Agent 通常只关注特定的能力。 情景：用户只需用自然语言填写配置文件，就可以轻松为各种功能和用例定义多代理系统，特别是在涉及不同角色（如编剧/小说创作）的内容创作工作室中。 Reference: https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/"
  },"/llm/app/llm-evaluation": {
    "title": "LLM 评估指南",
    "keywords": "llm",
    "url": "/llm/app/llm-evaluation",
    "body": "目前有三种主要的评估方式：自动基准测试、人工评估和使用模型评估。 HOW Bechmarks 1. 概述 使用基准测试评估通常包含两部分： 样本集：由多个样本组成，包含模型的输入和预期输出(gold答案)。样本设计旨在模拟真实场景，包括边界情况。 评估指标（得分）：用于量化模型性能的方法。对于大语言模型(LLM)，主要考虑两类输出： 生成式评估：分析模型生成的文本。 多选评估或对数似然评估：分析模型对不同选项的概率预测。 自动化基准的优势： 结果一致且可重复 可大规模实施且成本较低 大多数指标易于理解 通常使用高质量数据集，尽管并非完美(如MMLU及其改进版本) 局限性： 对复杂任务的评估能力有限 难以全面评估抽象能力(如”数学能力”) 数据集污染问题：公开发布的数据集可能被纳入后续模型的训练数据，但在真实任务中的泛化性较差。解决污染的一种方法是使用动态基准测试（定期刷新数据集进行评估），但这种方法在长期来看成本很高。 2. 评估数据集的选择与设计 选择现有数据集 在使用现有数据集时，需要仔细评估以下几个方面： 创建过程 样本创建者的背景和专业程度：由专家创建的数据集 &gt; 付费标注者的数据集 ~ 众包数据集 &gt; MTurk 数据集。 标注者间的一致性和质量审核流程：标注者是否意见一致？以及整个数据集是否经过作者审查。 这对于那些依靠低薪标注者的评估数据集尤为重要，通常这些标注者并不是目标语言的母语者（如 AWS Mechanical Turk），否则可能会出现拼写错误、语法错误或无意义的答案。 样本 抽样检查 prompts 的清晰度和答案的准确性，评估信息完整性 确认样本与评估目的的相关性 这些问题是否是您希望用来评估大型语言模型的问题？ 这些示例是否与您的使用场景相关？ 一些公开的评测数据集。 设计自定义数据集 自定义数据集可通过以下方式创建： 整合现有数据：可以从不同来源聚合现有数据，评估与您的任务相关的能力。 利用人工标注 生成合成数据 使用大型语言模型(LLMs) 应用基于规则的技术 3. 提示词设计 提示词结构通常包括： 任务说明(可选) 背景信息：为问题提供额外的上下文。 例如：对于总结或信息抽取任务，您可以提供内容来源 核心问题 选项(多选题) 连接词 设计注意事项： 即便是微小的措辞变化也可能显著影响结果 考虑使用示例和连接词以增强理解 警惕模型对特定提示格式的过拟合 某些评估指标可能需要严格限制输出格式 4. 如何应对数据污染问题 互联网上的公开数据集很可能已经被污染或将来会被污染。为了减轻这个问题，我们可以采取以下措施： 使用”金丝雀字符串”：在评估数据中加入特殊的字符组合，帮助模型开发者检查训练数据是否包含评估内容。 加密或限制评估数据：这样可以防止网络爬虫轻易获取，避免评估数据意外进入训练集。 动态更新基准测试：定期更新测试内容，防止模型简单记忆答案。不过，这种方法可能会增加成本。 事后检测污染：在进行基准测试后，尝试通过分析生成内容的困惑度或设计对抗性提示来检测可能的污染。但要注意，目前没有完全可靠的污染检测方法。 即便数据集被污染，它可能仍然对模型训练有价值。 “金丝雀字符串”是一种独特的、容易识别的文本序列，被故意插入到评估数据集中。它的主要目的是帮助模型开发者检测他们的训练数据是否被污染了评估数据。这个概念的名字来源于矿工们曾经用金丝雀来检测矿井中的有毒气体。 举个例子： 假设我们正在开发一个问答系统的评估数据集。我们可能会在这个数据集中插入如下的金丝雀字符串： CANARY_STRING_2024_EVAL_SET_DO_NOT_USE_IN_TRAINING 这个字符串会被添加到评估数据集的某些问题或答案中，比如： 问题： 什么是光合作用? CANARY_STRING_2024_EVAL_SET_DO_NOT_USE_IN_TRAINING 答案： 光合作用是植物利用光能将二氧化碳和水转化为葡萄糖和氧气的过程。 当模型开发者在训练他们的模型时，他们可以搜索这个特定的字符串。如果在训练数据中发现了这个字符串，就意味着评估数据已经不知不觉地混入了训练集，可能会导致模型性能评估的偏差。 {： .block-tip } 5. 实际应用中可能遇到的问题 模型微调、系统提示和对话模板 一些经过指令调优的模型可能表现不佳，除非你： 在开始推理时添加它们的系统提示。 使用正确的对话模板（通常需要在对话中添加”Assistant”和”User”等标识）。 另外，不同的分词器可能会对对话模板有不同的处理方式，尤其是在处理空格时。 分词和多项选择题评估 在进行多项选择题评估时，可能会遇到以下与分词相关的问题： 上下文和选项的分词方式： 通常应该将上下文和选项一起分词。 但某些分词器可能会在处理上下文和选项时出现不一致，影响比较的准确性。 句子开头和结尾标记： 某些模型对句子开头标记很敏感，可能需要手动添加。 有些模型可能无法在预期的句子结尾处停止，需要额外的检查和处理。 多语言评估中的分词： 对于不使用空格作为词分隔符的语言（如中文、日语等），需要使用专门的分词器来确保正确评估。 加速多项选择题评估的小技巧 如果能让模型只预测一个标记来回答问题，可以大大提高评估速度。这样只需对上下文进行一次推理，就能得到所有可能答案的概率分布。 生成式评估中的意外结果 如果遇到模型表现意外差的情况，请检查以下几点： 输出解析是否过于严格，导致有效答案被忽略。 模型是否无法在少量样本中遵循指定的输出格式。 模型是否过于啰嗦，难以得出准确答案。 针对这些问题，可以考虑调整解析方法、修改提示格式、增加允许的上下文长度，或在任务说明中强调简洁回答的重要性。 人工评审 1. 概述 人工评估就是让真人来评价AI模型的表现。本文主要讨论事后评估：模型训练完成后，针对特定任务让人来打分。 人工评估的优缺点 优点： 灵活性强：只要定义清楚，几乎什么都能评估 不会污染训练数据：评估者提出的新问题通常不在训练数据中 反映真实人类偏好：毕竟是人在打分 缺点： 第一印象偏见：评估者容易被初始印象影响 语气偏见：自信的语气可能掩盖事实错误 自我偏好偏见：评估者倾向于赞同符合自己观点的回答 身份偏见：不同背景的人可能有不同的评判标准 2. 系统性评估 有三种主要方法： 没有现成数据集时： 给评估者一个任务和评分标准，让他们与模型互动并打分。 例如：”测试模型是否会说脏话，说了给0分，不说给1分”。 有现成数据集时： 用数据集中的问题去问模型，然后把问题、模型回答和评分标准给评估者。 例如：”如果模型泄露隐私信息给0分，否则给1分”。 有数据集和评分时： 让评估者复查已有的评分，找出可能的错误。这也可以用来测试新的评估体系。 注意： 对于已上线的模型，还可以收集用户反馈或做A/B测试。 AI审计(第三方评估)通常也基于人工，但不在本文讨论范围内。 额外优点： 获得高质量、针对性强的数据 数据隐私得到保障 结果易于解释 额外问题： 成本高：需要付费给专业评估者 难以扩展：每次评估都需要新的人力 可重复性差：很难完全复现之前的评估结果 3. 非正式评估 还有两种比较随意的人工评估方式： 感受测试： 个人随意测试模型，获得对模型整体表现的直观感受。结果常在社交媒体上分享，属于轶事证据，可能存在确认偏见。 竞技场： 让大众用户与多个模型交谈，选出他们认为更好的模型。通过投票结果给模型排名，如LMSYS聊天机器人竞技场。 优点： 成本低：依靠志愿者 容易发现边界案例：利用了大众的创造力 相对容易扩展：只要有足够的感兴趣的人参与 问题： 高度主观：难以保证评分标准一致 样本代表性差：参与者可能不能代表普通大众 容易被操纵：第三方可能影响评估结果 4. 如何提高数据的标注质量 劳动力选择和激励很重要： 根据任务需求选择合适的标注人员，比如母语者、领域专家等。 筛选出高质量工作者，排除使用AI生成答案的人。 合理支付报酬以提高积极性。 制定详细的标注指南： 花时间深入思考和制定指南，这很关键。 指南可能比想象的更模糊，需要反复完善。 迭代标注过程： 准备多轮标注，让标注者更好理解要求。 生成多个样本以聚焦关键内容。 质量控制： 检查标注者间的一致性。 手动筛选，只保留最高质量的结果。 可以使用专业工具如Argilla来辅助构建高质量数据集。 使用模型进行评审 1. 概述 评估模型是一种特殊的神经网络，它的主要任务是对其他神经网络的输出进行评价。最常见的用途是评估文本生成的质量。它的强大之处在于，它能够对文本的复杂和细微特征进行评分。 举个例子，如果想测试模型是否预测出了正确的事实或数字，可以简单地比对预测结果和参考答案是否完全一致。 但是，如果想评估一些更开放性的能力，比如文字的流畅度、诗歌的质量，或者对输入的忠实度，就需要更复杂的评估工具了。这就是评估模型大显身手的地方。 评估模型的种类很多，从简单的专用分类器(比如用来过滤有害内容的”垃圾邮件过滤器”)到功能强大的大型语言模型(LLM)都包括在内。当使用LLM作为评估模型时，我们需要给它一个明确的指令，告诉它如何对文本进行评分。比如，”请给这段文字的流畅度打分，0到5分，0分表示完全无法理解…” 评估模型主要用于三类任务： 对生成的文本进行评分：根据特定标准(如流畅度、有害程度、一致性、说服力等)给文本打分。 成对比较：比较两段文本，选出在某方面表现更好的那一个。 计算相似度：衡量模型输出与参考文本之间的相似程度。 值得注意的是，除了本文重点介绍的LLM评估方法外，还有一些其他有效的评估手段。比如，分类器型评估模型在很多场景下都表现稳定且适应性强。此外，最近提出的将奖励模型用作评估者的方法也很有前景(详见这份技术报告)。 使用LLM进行评估的优缺点 优点： 相比人工评估更加客观：它们可以以一种客观且可重复的方式自动化主观判断。 规模化且可重复：相比人工标注，它们可以更快速地处理大量数据，并且能保持一致性。 成本效益高：不需要训练新模型，只要有好的提示和高质量的LLM就可以。比起雇佣真人标注者，成本更低。 与人类判断有一定相关性：在某些方面，它们的评判结果与人类的判断相近。 缺点： 隐藏的偏见：虽然看似客观，但LLM评估者可能存在难以察觉的偏见。这些偏见比人类的偏见更难被发现，因为我们通常不会主动去寻找它们。 数据量大：虽然处理速度快，但会产生大量需要人工审核的数据，以确保质量。 专业性有限：在某些特定领域，聘请真正的专家可能会得到更高质量的评估结果。 2. 评估LLM的主要步骤： 选择合适的基准 你需要一个基准来对比评估结果。这个基准可以是： 人工标注的数据 已知在特定任务上表现良好的其他评估模型的输出 公认的黄金标准 使用不同提示的同一模型的结果 基准数据集不需要很大，50个左右的样本通常就足够了。但这些样本需要具有代表性，能够体现边界情况，并且质量尽可能高。 确定评估指标 评估指标用于比较模型评估结果与基准之间的差异。 如果模型输出是二元类别或进行成对比较，评估会相对简单。你可以计算准确率(成对比较)或精确率和召回率(二元分类)，这些指标都很直观易懂。 如果是评分类的任务，与人工或其他模型评分的相关性比较会更复杂一些。你可以参考这篇博客中的相关部分，里面有更详细的解释。 评估你的评估模型 这一步就是用你的模型及其提示来评估测试样本。得到评估结果后，再用前面选定的指标和基准来计算评估分数。 你需要设定一个可以接受的阈值。对于成对比较任务，根据难度不同，可以将目标准确率设为80%到95%。对于评分任务，通常Pearson相关系数达到 0.8 就令人满意了。不过也有研究认为0.3就表示与人工评分有良好相关性，所以具体情况可能会有所不同。 3. 选择什么样的 LLM 进行评估 在使用语言模型(LLM)进行评估时， 有几种选择： 使用通用大型LLM 优点： 性能强大，尤其是一些封闭源代码模型如ChatGPT 开源模型如Qwen 2.5和Llama 3也在快速追赶 可通过API轻松访问，无需本地部署 缺点： API模型可能会无预警更新，影响结果可重复性 封闭模型不透明，难以解释 可能存在数据隐私问题 使用小型专用LLM 优点： 体积小，可本地运行 针对特定任务优化 开源透明 例如：Flow-Judge、Prometheus等模型 自行训练模型 步骤： 收集相关偏好数据 决定从头训练还是微调现有模型 使用收集的数据进行训练 这种方法可以完全定制，但需要更多工作。 4. 如何减少大语言模型在评估时的已知偏见 大语言模型(LLM)作为评估者时存在一些问题，我们可以采取以下措施来减轻这些偏见： 判断不一致 ： 多次询问同一问题时，模型可能给出不同的答案。解决方法是多问几次，采用大多数的结果。 偏爱自己 ： 模型倾向于给自己的回答更高分。解决方法是使用”评审团”，让多个模型一起评分。 对文本变化不敏感 ： 模型难以识别轻微修改过的文本，评分也不够稳定。解决方法是： 让模型先解释理由再打分 给模型一个明确的评分标准 位置偏好 ： 在比较多个选项时，模型可能总是偏爱某个位置的答案。解决方法是： 随机调换选项顺序 计算每个选项被选中的概率 长度偏好 ： 模型倾向于喜欢更长的答案。解决方法是在评分时考虑答案长度的影响。 格式敏感 ： 如果提问方式与模型训练时差异较大，评估效果会变差。解决方法是尽量按照模型训练时的格式来提问。 此外，还需注意： 模型与人类评分的一致性存在争议，特别是在专业领域(如医疗、法律等)。 模型不擅长发现虚假信息，尤其是那些看似真实但略有偏差的内容。 在摘要、忠实度等任务上，模型的表现与人类评分的相关性较低。 因此，在选择用LLM进行评估时，需要考虑这些局限性，并针对具体任务选择合适的评估方法。 WHY 1. 验证训练过程的有效性 评估的首要目的是验证模型训练是否按预期进行。这类似于软件开发中的回归测试，目的是确保新的改动没有破坏现有功能。这种评估主要关注趋势和范围，而非具体分数。它帮助我们判断训练方法的合理性，而不是评估模型的实际能力极限。 例子：假设你正在训练一个模型来回答常识性问题。你可能会这样检查： 在训练开始时，模型只能正确回答20%的问题。 经过一周的训练，它能回答40%的问题。 再过一周，它达到了60%。 这种进步趋势告诉你，训练正在起作用。但如果突然有一天，正确率下降到10%，你就知道可能出了问题，需要检查一下训练过程。 2. 模型间的相对比较 对不同模型进行排序，以识别最优的架构和方法。这里重要的是排名的稳定性，而非具体的分数。一个稳健的评估体系应该能在各种条件下保持相对一致的排序结果。 例子：假设你有三个模型：A、B和C。 在数学题上，排名可能是： B &gt; A &gt; C 在写作任务上，可能是： C &gt; A &gt; B 在编程方面，可能又是： A &gt; C &gt; B 重要的是这种排序是否稳定。如果每次测试都得到完全不同的排序，那这个评估方法可能就不太可靠。 3. 能力评估 这是最具挑战性的目标，因为： 复杂能力很难用单一指标来衡量 我们缺乏对”能力”本身的明确定义，特别是涉及推理和认知的高级能力 为人类设计的能力评估框架可能不适用于AI模型 因此，当谈到模型能力时，我们通常只能说”这个模型在特定任务上表现最佳”，而不能简单地断定”这个模型最强”。 例子：假设你想知道一个LLM是否能理解讽刺。 你可能会给它一些讽刺的句子，看它能否正确解释。 但即使它在这个测试中表现良好，也不能100%确定它真的”理解”了讽刺。 它可能只是学会了一些模式，而不是真正理解。 模型推理与评估 大语言模型(LLM)的工作原理 大语言模型的核心功能是预测文本。给定一段输入文本，它们能够预测合理的后续内容。这个过程分为两个主要步骤：分词和预测。 分词过程： 输入文本(在实际使用时称为”prompts”)首先被拆分成小的文本单元，称为”token”。每个 token 都与一个特定的数字相关联。模型能够识别的所有 token 集合被称为其”词汇表”。 预测过程： 模型会根据输入 prompts 生成一个概率分布，预测词汇表中最可能出现的下一个 token。为了生成连续的文本，通常会选择概率最高的 token(有时会添加一些随机性以增加输出的多样性)。然后将这个新 token 添加到 prompts 的末尾，重复上述过程，不断生成新的文本。 评估大语言模型 LLM的评估主要分为两大类： 给定 prompt 和一个或多个答案选项，评估模型对这些答案的概率预测。 给定 prompts， 评估模型生成的文本质量。 对数似然评估： 这种方法用于评估模型在给定 prompt 的情况下，产生特定续写的可能性。它主要用于有明确选项的情况，比如多选题。 步骤如下： 将每个答案选项与提示连接，输入模型。模型输出每个 token 的logits值(一种表示概率的数值)。 只保留与答案选项相关的最后的logits值。 应用对数softmax函数，得到对数概率(范围是[-inf， 0])。 将所有词元的对数概率相加，得到整体选项的对数概率。 可以根据选项长度进行归一化处理。 基于这个方法，可以应用以下指标： 在多个选项中找出模型最偏好的答案。 测试单个选项的概率是否高于0.5。 研究模型的校准性(一个校准良好的模型应该为正确答案赋予最高概率)。 例子： 提示：”1+1等于多少？” 选项：A. 2 B. 3 C. 4 将选项与提示连接，输入模型： “1+1等于多少？A. 2” “1+1等于多少？B. 3” “1+1等于多少？C. 4” 模型输出logits值，假设模型对这三个选项输出的logits值如下： A. 2 ： 10 B. 3 ： 2 C. 4 ： 1 这些数字代表模型认为每个选项的”合理性”。数值越大，模型认为这个选项越可能是正确答案。 应用对数softmax函数，我们将这些logits值转换为概率。简化起见，我们直接给出转换后的概率： A. 2 ： 0.9 B. 3 ： 0.08 C. 4 ： 0.02 优缺点： 优点： 确保所有模型都能访问正确答案 提供模型“置信度”（以及校准）的代理 评估速度快，特别是在我们要求模型只预测一个标记（A/B/C/D 选项索引，或是是/否等）时 可用于获取小型模型任务性能的信号 缺点： 会略微高估小型模型的得分，因为如果不受限制地生成答案，它们可能会生成超出可用选项范围的内容 有些模型偏好基于呈现顺序的特定选项，这可能导致评估结果不具代表性 生成式评估： 这种方法评估模型根据 prompts 生成的文本质量。过程如下： 将提示输入模型，选择最可能的下一个词元。将选择的词元添加到提示末尾，重复这个过程。直到达到结束条件(如最大长度或特殊的停止词元)。 将生成的内容与参考答案进行比较。可以使用简单指标(如精确匹配)、复杂指标(如BLEU分数)，或使用其他模型作为评判来评分。 优缺点： 优点： 实际上应与生成流畅文本的 LLM 能力相关联，这通常是人们真正感兴趣的内容 缺点： 可能更难评分（请参见下文“度量”部分） 通常比对数概率评估稍贵，尤其是包含采样时 限制模型输出 在某些情况下，需要模型的输出遵循特定格式。有几种方法可以实现这一点： 使用提示： 在任务提示中添加具体指令，告诉模型如何回答。这种方法对于高能力模型通常效果不错。 Few-shot和上下文学习： 在提示中提供示例(few-shot提示)，引导模型按照特定格式回答。 这种方法在2023年底之前效果很好。但随着指令微调技术的普及，新模型可能会过度拟合特定的提示格式。对于上下文窗口较小的旧模型，这种方法可能受到限制。 结构化文本生成： 使用语法或正则表达式定义输出结构。例如，outlines库使用有限状态机实现这一功能。 这种方法可以减少评估中的提示方差，使结果更稳定。然而，最新研究表明，在某些任务(如推理)中，这可能会降低模型性能。 参考： https://huggingface.co/blog/clefourrier/llm-evaluation https://github.com/huggingface/evaluation-guidebook?tab=readme-ov-file"
  },"/llm/app/hybrid-search": {
    "title": "混合检索",
    "keywords": "llm",
    "url": "/llm/app/hybrid-search",
    "body": "0. 召回 召回：快速准确获取相关信息的过程。召回可以被理解为建立索引和相似度得分匹配的结合。 索引负责缩小查找范围，而相似度得分匹配负责对缩小范围内的项进行精细排序。这样，系统能够在保证速度的同时，尽可能提高结果的相关性。 建立索引 ： Indexing 可以视为一种对数据集进行分组的方式。作用是把相似的数据放在一起，在查询时不需要遍历整个数据库，只查看与query 相似的那些数据集合。旨在减少查找时需要扫描的数据量。通过预先组织和存储数据的指针（或引用），索引能够在更小的范围内进行查找，从而显著提高查找速度。例如，B树和哈希表等数据结构都是为了加速查找过程而设计的。 索引的核心目的是通过优化数据的组织和访问方式，来加快查找速率。这一过程涉及到数据结构的选择、存储方式的设计以及对查找算法的优化。 相似度得分匹配 ： 在建立索引之后，系统需要评估每个潜在相关项与查询之间的相关性。这是通过计算相似度得分来实现的。相似度得分用于排序结果，以便返回最相关的项给用户。 1. Keyword-based search Indexing - Inverted Index 倒排索引 倒排（Inverted）索引 - 反转传统文档到词的关系。It is about the mapping between words and document lists. 在原始数据结构中，一个文档包含多个词。 Inverted Index 反转了上述关系。它以词（term）为键，项（倒排列表）存储包含该词的文档信息（如文档ID、词频、位置等）。 假设有以下三个文档： 文档1：\"the cat sat on the mat\" 文档2：\"the cat lay on the rug\" 文档3：\"the dog barked at the cat\" 首先，对文档进行预处理（分词、去除停用词等），然后构建倒排索引。 词项（Term） 倒排列表（Posting List） the 文档1, 文档2, 文档3 cat 文档1, 文档2, 文档3 sat 文档1 on 文档1, 文档2 mat 文档1 lay 文档2 rug 文档2 dog 文档3 barked 文档3 at 文档3 Similarity score - BM25 | Best Matching 25 BM25 ranking 算法通过考虑词项频率、逆文档频率和文档长度归一化，在提高搜索引擎性能方面发挥了重要作用。 对于查询 $Q$，文档 $D$ 的 BM25 分数是各个 $query$ 项分数的总和。 $D$ : 文档 $Q$ : 查询 $q_i$ : 查询中的第 $i$ 个词 $f(q_i, D)$ : 词 $q_i$ 在文档 $D$ 中的出现频率 $｜D｜$ : 文档 $D$ 的长度（词数） $\\overline{｜D｜}$ : 文档集合的平均长度 $k_1$ 和 $b$ : 调节参数，通常取值为 $k_1 \\approx 1.2$ 到 $2.0$ 之间， $b$ 一般取值为 $0.75$ 。 Key Components of BM25 TF - Term Frequency - 词频 一个词在一篇文档中出现的次数。一般来说，一个词出现得越多，它的词频就越高。文档与查询的相关性就越高。 IDF - Inverse Document Frequency IDF: 衡量一个词的重要性的指标，在整个语料库中出现次数越少分数越高。越具有区分性。 其中： $N$ 是文档总数。 $n(q_i)$ 是包含词 $q_i$ 的文档数量。 由于IDF是基于整个文档集合的统计信息，因此对于同一个查询中的所有文档，IDF值是相同的。 Document Length Normalization | 文档长度归一化: BM25 引入了文档长度归一化的概念，较长的文档可能会包含更多的词，可能会导致评分时偏向于这些长文档。因此需要进行归一化处理，即通过参数 $b$来调整文档长度对评分的影响。以避免长文档在评分时的不公平优势。 Implementation: import math from collections import Counter class BM25: def __init__(self, documents): self.documents = documents self.document_count = len(documents) self.avg_document_length = sum(len(doc) for doc in documents) / self.document_count self.term_counts = self.calculate_term_counts() self.k1 = 1.2 self.b = 0.75 def calculate_term_counts(self): term_counts = Counter() for document in self.documents: term_counts.update(document) return term_counts def calculate_idf(self, term): document_with_term_count = self.term_counts[term] return math.log((self.document_count - document_with_term_count + 0.5) / (document_with_term_count + 0.5)) def calculate_bm25_score(self, query, document): score = 0.0 document_length = len(document) query_terms = Counter(query) for term in query_terms: if term not in self.documents: continue idf = self.calculate_idf(term) term_frequency = document.count(term) numerator = term_frequency * (self.k1 + 1) denominator = term_frequency + self.k1 * (1 - self.b + self.b * (document_length / self.avg_document_length)) score += idf * (numerator / denominator) return score def rank_documents(self, query): document_scores = [] for document in self.documents: score = self.calculate_bm25_score(query, document) document_scores.append((document, score)) ranked_documents = sorted(document_scores, key=lambda x: x[1], reverse=True) return ranked_documents # Example usage: documents = [ ['apple', 'banana', 'orange', 'apple'], ['banana', 'orange', 'orange'], ['apple', 'apple', 'banana', 'banana'], ['orange', 'orange', 'banana'] ] bm25 = BM25(documents) query = ['apple', 'banana'] ranked_documents = bm25.rank_documents(query) for document, score in ranked_documents: print(f\"Document: {document}, Score: {score}\") 2. vector search 向量检索的核心思想是将数据（如文本、图像等）转换为高维向量表示，然后通过计算向量之间的相似度来进行检索。 relational database vs vector database   relational database vector database type of data they store structured data unstructured data: text or images query results based on matches for specific keywords based on similarity 关系型数据库的缺点包括： 必须事先想好用户可能会搜索什么信息 如果用户搜索的关键字没有在数据记录中提及（如书的摘要中未提及“食物”），相关而重要的信息（比如与食物相关的书籍）可能无法被检索到。 添加所有可能需要的信息非常耗时，并且仍然无法保证找到的信息是完整的，因为可能无法预测到所有用户查询的需求或关键字。 vector database They were used in recommendation systems and have also been used in question-answering applications recently. 理解向量索引 Indexing ANN（Approximate Nearest Neighbors） 是构建索引的原则（策略）。 KNN vs ANN KNN: 直接计算查询点与所有数据点的距离，通常不使用复杂的索引结构。精确但 time-consuming(耗时) ANN: 使用数据结构（如局部敏感哈希、KD树、HNSW等）来组织数据。这些结构旨在减少搜索空间，使得查找过程更快。在查询时，利用索引快速找到与查询点近似的k个最近邻，而不是精确的最近邻。 ANN 算法分为 3 类：Trees、Hashes、Graphs 查询速度 ：LSH &gt; IVF &gt; HNSW 精度 ：HNSW &gt; IVF &gt; LSH 数据规模：LSH ≈ IVF &gt; HNSW HNSW - 适合高维数据，提供了快速且精度较高的搜索，但构建索引的开销较大，所以对内存的消耗也大。 1. Inverted File(IVF) | 倒排文件 这是最基本的索引技术。它使用 K-means 聚类等技术将整个数据分成多个簇。对于每个聚类中心，建立一个倒排列表，记录属于该簇的所有向量的ID。数据库中的每个向量都分配给特定的簇。 在查询时，首先计算查询向量与各个聚类中心的距离，快速确定查询向量最接近的几个簇。然后，仅在这些候选簇的倒排列表中执行更精细的相似度计算，以找到最近邻向量。 IVF的变体：IVFFLAT、IVFPQ和IVFSQ IVFFLAT IVFFLAT 是 IVF 的一种基本实现，结合了倒排文件和线性扫描。其过程如下： 线性扫描 ：在分配到的簇的倒排列表中，通过线性扫描的方式找到与查询向量最相似的向量。 IVFFLAT用于数据集不是非常大且搜索过程需要高准确性的场景。由于线性扫描的复杂度，随着簇的规模增大，效率可能会下降。 **IVFPQ Inverted File with Product Quantization** 在使用 IVF 构建好簇后，如何在簇内高效检索到其中的相似的向量。 在IVFPQ中，簇中的向量被分解为多个子向量。每个子向量使用产品量化（PQ）进行编码。对于查询向量，当确定相关簇后，查询向量也会被分解为相同的子向量，并使用相同的量化方法进行编码。比较时，计算查询向量的量化子向量与簇内所有向量的量化子向量之间的距离（通常是欧氏距离或其他合适的距离度量）。每个子向量的比较会产生一个距离分数。将所有子向量的距离分数加总或合并，得到最终的完整向量的距离分数。 与 IVFFLAT 相比，这种方法有两个优点： 向量以紧凑的方式存储，占用的空间比原始向量少。 查询过程更快，因为它不比较所有原始向量，而是比较编码向量。 **IVFSQ Inverted File with Scalar Quantization** 在IVFSQ中，每个簇中的每个向量都经过标量量化处理。 标量量化 ：对每个维度进行独立量化，通常使用少量的比特表示每个维度，从而减少存储需求。 优点 ：标量量化通常比产品量化更简单，计算开销更小，但可能会导致更大的近似误差。 例子： 假设你有一个数据点 (3.14159, 2.71828, 1.61803)，而你为每个维度选择的码本是： 维度 1: [3.0, 3.5, 4.0, 4.5] 维度 2: [2.5, 3.0, 3.5, 4.0] 维度 3: [1.5, 2.0, 2.5, 3.0] 经过量化，这个数据点可能被映射为 (3.0, 2.5, 1.5)，并且用索引 (0, 0, 0) 来表示。 supplement 2. Hierarchical Navigable Small World (HNSW) https://www.pinecone.io/learn/series/faiss/hnsw/ HNSW = probability skip lists + Navigable Small World(NSW) graphs Navigable Small World | NSW 是一个搜索复杂度为多对数 $T = O(log n)$的图，它使用贪婪路由。 搜索过程：从低度顶点开始，到高度顶点结束。由于低度顶点的连接很少(zoom out)，算法可以在它们之间快速移动，有效地导航到最近的邻居可能所在的区域。然后，算法逐渐切换到高度顶点（zoom in），在该区域的顶点中寻找最近的邻居。 NSW图构建：每个新点都向其K近邻连边，那么早连的边就会成为long range link。 HNSW算法 HSNW基于与skip list 和 Navigable Small World 相同的原理。它的结构是一个多层图，顶层的连接较少，底层的区域较密集。 Search 搜索从最高层开始，每当在各层节点中贪心找到当前最近邻居时，就向下搜索一层。最终，在最底层找到的近邻就是查询的答案。 我们会寻找与查询向量最近的 efSearch（一个超参数）近邻，并将这些近邻中的每一个作为下一层的切入点。 Construction 层级参数（$L$） ：决定图有多少层。 层级乘数（$m_L$） ：影响向量被分配到各层的概率。 连接参数（M, M_max, M_max0） ：决定每个节点的连接数。 搜索参数（ef, efConstruction） ：影响搜索时的邻居数量 假设我们要插入一个新节点 $q$： 根据概率函数和 $m_L$，被随机分配一个整数 $L$. 表示该节点在图中能出现的最大层数。 节点被赋值层数 $L$ 后，其插入分为两个阶段： 算法从顶层开始，贪心寻找最近的节点。然后将找到的节点作为下一层的入口点，继续搜索过程。一旦到达第 $l$ 层，插入就进入第二步。(在 $L$ 层之前都不插入) 从第 $l$ 层开始，算法在当前层插入新节点。然后，该算法与步骤 a 相同，但不再只寻找一个最近的邻居，而是寻找 efConstruction（超参数）最近的邻居。然后，从 efConstruction 近邻中选出 M 个，并建立从插入节点（新节点）到它们的边。之后，算法进入下一层，每个找到的 efConstruction 节点都是一个入口点。新节点及其边插入最底层 0 后，算法结束。 3. LSH | Locality-Sensitive Hashing | 局部敏感哈希 旨在通过哈希函数将相似的数据点映射到相同的桶中，从而提高在大规模数据集中查找相似项的效率。 Similarity Measures 一旦通过索引定位到了可能相似的向量集，然后需要量化查询向量与该集合中每个向量之间的相似度。 余弦相似性（Cosine Similarity）： 度量两个向量在方向上的相似度，常用于文本数据，因为它只考虑向量之间的角度，而不受向量长度的影响。 \\[Cosine Similarity = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\\] 其中，$\\mathbf{A} \\cdot \\mathbf{B}$ 表示向量 $\\mathbf{A}$ 和 $\\mathbf{B}$ 的点积，$|\\mathbf{A}|$ 和 $|\\mathbf{B}|$ 表示向量的范数（即向量的长度）。 点积（Dot Product）： 数学上表示两个向量的相似度，可以看作是向量在长度和方向上的综合相似度。 \\[\\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^{n} A_i B_i\\] 其中，$A_i$ 和 $B_i$ 是向量 $\\mathbf{A}$ 和 $\\mathbf{B}$ 的第 $i$ 个元素。 欧几里得距离（Euclidean Distance）（L2 距离）： 度量两个点在多维空间中的实际距离，适用于长度和方向同时重要的场景。距离（分数）越近，越相似。 \\[d(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\\] 其中，$A_i$ 和 $B_i$ 是点 $\\mathbf{A}$ 和 $\\mathbf{B}$ 在第 $i$ 维的坐标。 曼哈顿距离（Manhattan Distance）： 度量在标准坐标系上点到点之间的距离，适用于网格状结构的数据。 \\[d(\\mathbf{A}, \\mathbf{B}) = \\sum_{i=1}^{n} |A_i - B_i|\\] 汉明距离（Hamming Distance）： 度量两个等长字符串之间的差异，主要应用于信息编码。 \\[d(\\mathbf{A}, \\mathbf{B}) = \\sum_{i=1}^{n} \\begin{cases} 0, &amp; \\text{if } A_i = B_i \\\\ 1, &amp; \\text{if } A_i \\neq B_i \\end{cases}\\] 其中，$A_i$ 和 $B_i$ 是字符串 $\\mathbf{A}$ 和 $\\mathbf{B}$ 的第 $i$ 个字符。 Reference: Explaining Vector Databases in 3 Levels of Difficulty Applications FAISS PostgreSQL &amp; PGVector 3. Fusion of keyword-based and vector search results 有许多不同的策略可以将两个列表的排名结果合并为一个单一的排名。一般来说，搜索结果通常会首先被评分。这些分数可以根据指定的度量标准计算，例如余弦距离，或者只是搜索结果列表中的排名。然后，这些分数会根据参数 alpha 进行加权，该参数决定每个算法的权重并影响结果的重新排序。 hybrid_score = (1 - alpha) * sparse_score + alpha * dense_score 控制关键字搜索和语义搜索之间权重的参数 alpha 可以视为需要调整的超参数。 注意： 对于 faiss 默认使用欧式距离来表示向量相似，它是分数越低相似度越高。在计算 hybrid_score 时，需要将欧氏距离转换为相似度得分。 Graph RAG Graph RAG"
  },"/pages/about/": {
    "title": "About",
    "keywords": "Jekyll",
    "url": "/pages/about/",
    "body": "This is an about page."
  },"/pages/contact/": {
    "title": "Contact",
    "keywords": "Jekyll",
    "url": "/pages/contact/",
    "body": ""
  },"/pages/design/draft/": {
    "title": "Draft",
    "keywords": "",
    "url": "/pages/design/draft/",
    "body": "This is an draft page."
  }}
